---
title: Hypothesis Driven Prioritization
date: 2024-12-08T21:41:05.959Z
image: /images/uploads/hypothesis.png
description: Hypothesis Driven Prioritization
---
**Hypothesis Driven Prioritization and the Scientific Method** 

We will use the hypothesis-driven prioritization framework, leveraging the scientific method and agile testing tactics to prioritize, execute and iterate.  You’ve set your strategy and gained alignment with leadership.  Now, the question of what we’re doing to get there comes up.  There are so many problems out there that we can go after, both known and unknown, each with different customer and business impact.  Identifying the problems we should prioritize, and what solutions we use test our hypotheses will be the difference between ambiguous results and step-function growth.   

 

Hypothesis-driven prioritization is the process of researching and deeply understanding the customer and their pain points.  With this holistic picture of the customer, we can use data to deduce and articulate clear problem statements.  For each problem statement, we generate a specific, measurable, attainable, relevant (to strategy) and time-bound (quarter) hypothesis – a loose form of the SMART framework.  At this point, it is tempting to jump into solutioning.  However, with a growing set of customer problems, we can use data and proxies to estimate the total addressable market of the problem – if we were to alleviate 100% of the pain, what would the value be?   

 

Equipped with your sized problem statements, we can make more educated choices for what problems to go after.  The depth of thought and concise problem statements set constraints on solutioning that have a higher probability of solving each unique problem.  Moreover, they limit scope creep and encourage scrappiness.  High impact and high velocity are possible.  We will review situational tools and strategies for solutioning and execution. 

 

Hypothesis-driven prioritization shares a lot of the rigor of an academic thesis but in an empirical setting.  Diligence in decision making is more important than each decision itself.  If you can master this approach, you will be unstoppable.  The following sections will break down the components listed above. 

 

**Observations** 

Creating great customer experiences that drive business results requires a deep understanding of the customer and what matters to them.  Ideas are cheap – choosing the right ideas isn’t.  To prioritize our roadmap, we will apply a structured approach to discovery before generating problem statements.  Utilizing diverse discovery methodologies will yield a more holistic view of customer problems: 

 

**Traditional Qualitative** 

* Published best practices (academic, tech blogs, usability research) 
* UXR (surveys, user testing, codesign, interviews) 
* Customer problem exercises (JTBD, user journeys, etc.) 
* Eating your own dog food (using the app, working in the store, etc.) 

 

**Scaled Qualitative** 

* Session tracking (Optimizely, etc.) 
* Supervised eye/emotional tracking 
* UGC review analysis 
* Customer complaints 

 

**Competitive**  

* Reverse engineering (competitors, adjacent industries, standouts) 
* Earning statements, publications or speaking events by competition 
* Job descriptions 
* Non-sector trends (Amazon, Wayfair, Publix, etc.) 
* Product and tech blogs 

 

**Quantitative** 

* Funnel depth analysis 
* Product mix 
* Errors 
* Completion 
* Post purchase analysis 
* Behavior of new/loyal/return purchasers 
* Omni experience data  



It will take time for us to gather and maintain data across these discovery methodologies – the expectation is not that we have this all prepared immediately.  However, the application and use of each methodology is important.   

 

Qualitative and scaled qualitative research are very effective at identifying potential problems and getting feedback on potential solutions.  On the flip side, the sparsity of data and possible confirmation bias often make these signals less useful for prioritization or conceptualization. 

 

Quantitative, and in some cases scaled qualitative research, are very powerful for identifying user frictions.  The expansiveness of the datasets is especially helpful in sizing the opportunity and bringing focus to solutioning.  A caveat is that raw data doesn’t account for the emotional response of a user: Is this problem important? How would they react emotionally? Are we changing our natural behavior? 

 

Competitive research is more than just looking at what another company is doing.  It’s breaking down “why they did it” and the problem they were attempting to solve.  You’re essentially borrowing all their hard work identifying the problem and analyzing the effectiveness of their solution.  This type of research shouldn’t be restricted to direct competitors; there are mature tech companies and eCommerce companies we can learn from, too. 

 

There is a significant upfront investment to understanding the customer.  It will pay dividends in the short-run and be easier to maintain in the long-run.  As we learn, we update our observations and reconcile our prioritization against them in an iterative process.  The most important takeaway is that one methodology alone introduces risk of less impactful prioritization or suboptimal solutions.  Combining methodologies gives a holistic view, allowing us to take bigger bets with higher confidence.   

 

**Problem Statement** 

A problem statement must be a true user pain point – if you’re not solving a real problem, you cannot anticipate impact.  A user can be a wide array of players: the guest, the franchisee, an employee, a corporate stakeholder, etc.  Determining who to focus on can sometimes be arbitrary and you may need to lean on your OKRs (e.g. profitability) or a core principle (e.g. customer first).

 

The problem statement should highlight the core customer friction, how that friction impacts their behavior and the measurable impact of the problem.  It should be concise to keep the team focused and refine solutions.  If it is too broad or vague, you’ll have scattershot solutions that may not address the heart of the issue.  When you’re struggling to make something concise, break it into pieces to see if there are multiple problems. 

 

**Examples of well-structured problem statements** 

* For a Ride-Sharing App: “Users in suburban areas are experiencing longer wait times than in urban areas, leading to a 30% decrease in ride bookings in these regions.” 
* For a Project Management Tool: “Project managers find it challenging to track team progress in real-time, leading to delays in project delivery and a 20% decrease in client satisfaction.” 
* For an Online Learning Platform: “Learners are not completing courses due to a lack of engagement and interactivity, resulting in a 50% decrease in course completion rates.” 



**A poorly written customer problem statement** 

Customers want to be able to pay bills using their smartphone camera.  This is a poor customer problem statement for a project to build a feature which enables customers to pay bills using their smartphone cameras. It is not a customer problem, and neither is it a meaningful definition of an opportunity to do something better. The problem needs to be framed as a problem with the status quo, a limitation with the way customers currently pay bills. 

 

**A well written customer problem statement** 

Customers find paying bills tedious and the process of entering the details for each bill cumbersome and time consuming.  You can improve this by adding in the specific customers who have the problem (which you learned from research): Customers who use their mobile phones for banking find paying bills tedious and the process of entering the details for each bill cumbersome, error prone and time consuming. 

 

**Components of a strong problem statement** 

1. It does not presuppose a solution 
2. It targets a specific customer group with whom you can build empathy 
3. It reflects the actual user experience 
4. It directs the project to solve the underlying problems 
5. It provides measurable obstacles (problems) to overcome and thus a way to measure the success of your proposed solutions and learn how to improve them 

 

Over time, you will refine your ability to craft great problem statements.  Remember, if you struggle to concisely articulate the problem, consider breaking it down more.  When you master this skill, you’ll have more impactful, more predictable, and more focused outcomes. 

 

**Hypothesis** 

In general, a hypothesis is a statement of fact that we will be testing; It should not begin with “we believe”.  It should not be a solution (e.g., “if we build…”). It should be measurable within the testing period.  For example, NPS is a great long-run metric for brand loyalty and growth, but it’s near impossible to measure in an AB test.  If we reuse our example for drop chart adoption by a store employee, we may say something like:  

 

Problem Statement: “The logistical difficulty for an employee requesting more chicken from the fry cook lowers the frequency of requests to the fry cook, leading to more out-of-stock protein.”  Here, we have the users (packing and fry), we have the problem (communicating needs) and the outcome (out-of-stock protein). 

 

Hypothesis: “Improving the communication of protein stock needs to the fry cook will lead to less out-of-stock protein.”  Here we have the user (the one that fries), we have the improvement (knowing when to fry) and the outcome (less out-of-stock).  Note, we intentionally did not include the person requesting additional protein.  The tool today requires the intervention of the packager to ask for more, but a future solution may no longer need them to communicate.  This gets into more of an art than a science, but a well framed hypothesis can change the outcome of your ideation sessions. 

 

**Impact Sizing** 

For impact sizing when the company goals are business oriented, it can be difficult to define the right success metrics for a project.  Your primary success metric should be in your control.  Some higher-level metrics, including OKRs, are volatile as they have many impacts outside of your domain (like pricing team, discounts, marketing, etc.).  To stay customer focused while contributing to our company goals, you need to understand (at least) three things: what metrics are in your control that ladder up into OKRs, (2) can you measure your impact in a reasonable amount of time (AB test), and (3) which metrics do you have the most influence over (if I turn this knob, this other things happens).  Understanding our influence is why we often “test to learn, not test to earn”.  The importance of choosing the right success metrics cannot be understated. 

 

Generally, to keep focused, sticking to 1 primary success metric is recommended.  Occasionally, you may have 1-2 secondary success metrics.  These can be “do no harm” metrics like conversion or complimentary “adoption”.  Conversion is tightly coupled to OKRs and even if your primary success metric is successful, it could damage our OKRs goals.  On the flip side, your test could be highly successful, but the adoption rate is so low the total impact is muted.  Note, this does not mean you’re ignoring other metrics; rather, you will use other metrics to determine why your primary metrics moved. 

 

**When setting your metrics and evaluating impact, lean heavily into your observations:**  

* What is the audience size that would be exposed to the change or are currently experiencing friction 
* What of that friction does your proposed test(s) really solve?  
* What part of the problem will not be addressed? 
* Are your metrics financial, customer or quality focused?  A few examples: 
* Financial: Revenue, gross profit, Opex, productivity/efficiency 
* Customer: Engagement, conversion, bounce, retention, new/repeat, AOV 
* Quality: performance, stability, errors 

 

One common mistake is to use correlation metrics without a view into the magnitude of impact: “a 1% increase in promo use leads to 0.5% increase in conversion”.  This number is helpful in translating a secondary metric into a primary metric but does not address how much potential impact we can have to promote use.  Put more focus on if the problem is big enough to matter – without that, correlations can be easily misinterpreted.  Finally, ensure you can express your logic of how you got to your projection.